{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1364s\u001b[0m 3s/step - accuracy: 0.2191 - loss: 1.9402 - val_accuracy: 0.2471 - val_loss: 1.8203\n",
      "Epoch 2/2\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 1s/step - accuracy: 0.2437 - loss: 1.8222 - val_accuracy: 0.2471 - val_loss: 1.8146\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================================= TEXT MODALITY ==========================================================================================================\n",
    "\n",
    "# Load text dataset\n",
    "text_csv_path = r'C:\\Users\\ssid7\\Desktop\\Projects\\MCA_Major_Project-Multimodal_Emotion_Recognition_System\\tweet_emotions.csv'\n",
    "df = pd.read_csv(\n",
    "    text_csv_path,\n",
    "    usecols=['content', 'sentiment'],  # Only load necessary columns\n",
    "    dtype={'content': 'string', 'sentiment': 'category'}\n",
    ")\n",
    "df = df.rename(columns={'content': 'tweet', 'sentiment': 'label'})\n",
    "\n",
    "# Filter and preprocess labels\n",
    "labels = ['neutral', 'sadness', 'happiness']\n",
    "df = df.query('label in @labels')\n",
    "df['label'] = df['label'].replace({'sadness': 'negative', 'happiness': 'positive'})\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)  # 3 classes: negative, neutral, positive\n",
    "text_model.eval()\n",
    "\n",
    "# Encode labels for text\n",
    "label_encoder_text = LabelEncoder()\n",
    "label_encoder_text.fit(['negative', 'neutral', 'positive'])\n",
    "\n",
    "# Text prediction function\n",
    "def predict_emotion_from_text(text):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = text_model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return label_encoder_text.inverse_transform([predicted_class])[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================================================= IMAGE MODALITY =======================================================================================================\n",
    "\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = r'C:\\Users\\ssid7\\Desktop\\Projects\\MCA_Major_Project-Multimodal_Emotion_Recognition_System\\data\\train'\n",
    "test_dir = r'C:\\Users\\ssid7\\Desktop\\Projects\\MCA_Major_Project-Multimodal_Emotion_Recognition_System\\data\\test'\n",
    "\n",
    "# Image data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0, rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Load training and testing image data\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Define VGG16 model for image emotion recognition\n",
    "def create_vgg_model(num_classes):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)  # Adjust output layer to match number of classes\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = train_data.num_classes\n",
    "\n",
    "# Initialize and train the model\n",
    "image_model = create_vgg_model(num_classes)\n",
    "image_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train and save the model weights\n",
    "image_model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=2,\n",
    ")\n",
    "image_model.save_weights(\"vgg_emotion_model.weights.h5\")\n",
    "\n",
    "# Load pre-trained image model\n",
    "image_model.load_weights(\"vgg_emotion_model.weights.h5\")\n",
    "\n",
    "# Encode labels for images\n",
    "label_encoder_image = LabelEncoder()\n",
    "label_encoder_image.fit(list(train_data.class_indices.keys()))\n",
    "\n",
    "\n",
    "\n",
    "# Image prediction function\n",
    "def predict_emotion_from_image(image_path):\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    img = image.load_img(image_path, target_size=(48, 48))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    with tf.device('/CPU:0'):  # Ensure the prediction runs on the CPU\n",
    "        predictions = image_model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "    return label_encoder_image.inverse_transform([predicted_class])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================== MULTIMODAL FUSION=====================================================================================================================\n",
    "\n",
    "# Fusion function combining text and image predictions\n",
    "image_path = r'C:\\Users\\ssid7\\Desktop\\Projects\\MCA_Major_Project-Multimodal_Emotion_Recognition_System\\data\\test\\happy\\PrivateTest_647018.jpg'\n",
    "text = \"Buying new clothes in discount\"\n",
    "def predict_multimodal_emotion_batch(texts, image_paths):\n",
    "    all_predictions = []\n",
    "\n",
    "    # Process texts\n",
    "    for text in texts:\n",
    "        try:\n",
    "            text_emotion = predict_emotion_from_text(text)\n",
    "            print(f\"[Text] '{text}' -> {text_emotion}\")\n",
    "            all_predictions.append(text_emotion)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in text prediction: {e}\")\n",
    "\n",
    "    # Process images\n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            image_emotion = predict_emotion_from_image(img_path)\n",
    "            print(f\"[Image] '{img_path}' -> {image_emotion}\")\n",
    "            all_predictions.append(image_emotion)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in image prediction: {e}\")\n",
    "\n",
    "    # Count most frequent emotion(s)\n",
    "    counter = Counter(all_predictions)\n",
    "    most_common = counter.most_common()\n",
    "\n",
    "    # Handle tie\n",
    "    if len(most_common) > 1 and most_common[0][1] == most_common[1][1]:\n",
    "        top_emotions = [emotion for emotion, count in most_common if count == most_common[0][1]]\n",
    "        return f\"Tie between: {', '.join(top_emotions)}\"\n",
    "    else:\n",
    "        return f\"Final Predicted Emotion: {most_common[0][0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text] 'I just got a promotion at work!' -> positive\n",
      "[Text] 'Feeling a bit low today.' -> positive\n",
      "[Text] 'Enjoying a relaxing weekend.' -> positive\n",
      "[Text] 'So frustrated with the traffic!' -> positive\n",
      "[Text] 'Watching my favorite movie.' -> positive\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step\n",
      "[Image] 'data\\test\\fear\\PrivateTest_2159049.jpg' -> happy\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "[Image] 'data\\test\\happy\\PrivateTest_258543.jpg' -> happy\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "[Image] 'C:\\Users\\ssid7\\Desktop\\Projects\\MCA_Major_Project-Multimodal_Emotion_Recognition_System\\data\\test\\happy\\PublicTest_99753173.jpg' -> happy\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "[Image] 'C:\\Users\\ssid7\\Desktop\\Projects\\MCA_Major_Project-Multimodal_Emotion_Recognition_System\\data\\test\\happy\\PublicTest_98334368.jpg' -> happy\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "[Image] 'C:\\Users\\ssid7\\Desktop\\Projects\\MCA_Major_Project-Multimodal_Emotion_Recognition_System\\data\\test\\happy\\PublicTest_99849498.jpg' -> happy\n",
      "Tie between: positive, happy\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================================= EXAMPLE USAGE ======================================================================================================================\n",
    "\n",
    "texts = [\n",
    "    \"I just got a promotion at work!\",\n",
    "    \"Feeling a bit low today.\",\n",
    "    \"Enjoying a relaxing weekend.\",\n",
    "    \"So frustrated with the traffic!\",\n",
    "    \"Watching my favorite movie.\"\n",
    "]\n",
    "\n",
    "image_paths = [\n",
    "    r'data\\test\\fear\\PrivateTest_2159049.jpg',\n",
    "    r'data\\test\\happy\\PrivateTest_258543.jpg',\n",
    "    r'C:\\Users\\ssid7\\Desktop\\Projects\\MCA_Major_Project-Multimodal_Emotion_Recognition_System\\data\\test\\happy\\PublicTest_99753173.jpg',\n",
    "    r'C:\\Users\\ssid7\\Desktop\\Projects\\MCA_Major_Project-Multimodal_Emotion_Recognition_System\\data\\test\\happy\\PublicTest_98334368.jpg',\n",
    "    r'C:\\Users\\ssid7\\Desktop\\Projects\\MCA_Major_Project-Multimodal_Emotion_Recognition_System\\data\\test\\happy\\PublicTest_99849498.jpg'\n",
    "]\n",
    "\n",
    "final_result = predict_multimodal_emotion_batch(texts, image_paths)\n",
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
